Seamless integration between the Whisper Automatic Speech Recognition (ASR) library and the OpenAI GPT API. This web page allows users to record a an audio clip from your browser, transcribe the recorded speech into text using Whisper, and then use the GPT API to generate an appropriate response based on the transcribed text, and last speak the output to you.

functionalities:

* The record_audio function captures audio input from the user's microphone and saves it as a WAV file. Audio stop to be recorded after 1 second of silence
* The transcribe_audio function utilizes the Whisper ASR library to transcribe the recorded audio into text. It also detects the spoken language and displays it.
* The ask_gpt function sends the transcribed text as a prompt to the OpenAI GPT API and receives a generated response based on the input.
* The script then prints the GPT-generated response for the user to see.
* The main advantage of this script is that it allows users to interact with the GPT API simply by speaking, without the need to manually type in their questions or prompts.
* During the image creation all the basic model for whisper are downloaded. If you want large or medium you need to change that
* By default GPT accept every language but current speak is only in english



remember to add the GPT API key in you env first

export  OPENAI_API_KEY=......


You only need to say what you want to ask the GPT API.

To compile the image you need to do 

 docker build -t audio-to-gpt .  

and to execute 

docker run -p 5001:5000 -e OPENAI_API_KEY=$OPENAI_API_KEY audio-to-gpt  

and after that open your browser in

https://127.0.0.1:5001

and enjoy


Remember that depends of your computer, lambda, cloud run, etc resources spead will be different
